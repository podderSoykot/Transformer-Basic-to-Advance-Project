{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Using cached torch-2.3.1-cp310-cp310-win_amd64.whl.metadata (26 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\windows 10 pro\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\windows 10 pro\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.8.0)\n",
      "Collecting sympy (from torch)\n",
      "  Using cached sympy-1.12.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\windows 10 pro\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.0.3)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2024.6.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting mkl<=2021.4.0,>=2021.1.1 (from torch)\n",
      "  Using cached mkl-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.4 kB)\n",
      "Collecting intel-openmp==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached intel_openmp-2021.4.0-py2.py3-none-win_amd64.whl.metadata (1.2 kB)\n",
      "Collecting tbb==2021.* (from mkl<=2021.4.0,>=2021.1.1->torch)\n",
      "  Using cached tbb-2021.12.0-py3-none-win_amd64.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\windows 10 pro\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jinja2->torch) (2.0.1)\n",
      "Collecting mpmath<1.4.0,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Downloading torch-2.3.1-cp310-cp310-win_amd64.whl (159.8 MB)\n",
      "   ---------------------------------------- 159.8/159.8 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading mkl-2021.4.0-py2.py3-none-win_amd64.whl (228.5 MB)\n",
      "   -------------------                      112.4/228.5 MB 1.3 MB/s eta 0:01:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Sample text corpus\n",
    "text_corpus = \"\"\"\n",
    "    Transformers are a type of model architecture used for natural language processing tasks.\n",
    "    They have become the model of choice for many NLP tasks due to their performance and flexibility.\n",
    "    \"\"\"\n",
    "\n",
    "# Preprocessing the text\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)\n",
    "    text = re.sub(\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "# Tokenizing the text\n",
    "def tokenize(text):\n",
    "    return text.split()\n",
    "\n",
    "# Preprocess and tokenize the corpus\n",
    "processed_text = preprocess_text(text_corpus)\n",
    "tokens = tokenize(processed_text)\n",
    "\n",
    "# Create a vocabulary\n",
    "vocab = list(set(tokens))\n",
    "word_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "idx_to_word = {idx: word for word, idx in word_to_idx.items()}\n",
    "\n",
    "# Encode the tokens\n",
    "encoded_tokens = [word_to_idx[word] for word in tokens]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_seq_length, d_model))\n",
    "        self.transformer = nn.Transformer(d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward)\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding(src) + self.positional_encoding[:, :src.size(1), :]\n",
    "        tgt = self.embedding(tgt) + self.positional_encoding[:, :tgt.size(1), :]\n",
    "        src = src.permute(1, 0, 2)  # [seq_len, batch_size, d_model]\n",
    "        tgt = tgt.permute(1, 0, 2)  # [seq_len, batch_size, d_model]\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output.permute(1, 0, 2))  # [batch_size, seq_len, vocab_size]\n",
    "        return output\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = len(vocab)\n",
    "d_model = 64\n",
    "nhead = 8\n",
    "num_encoder_layers = 3\n",
    "num_decoder_layers = 3\n",
    "dim_feedforward = 256\n",
    "max_seq_length = 10\n",
    "\n",
    "# Instantiate the model\n",
    "model = TransformerModel(vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, dim_feedforward, max_seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, encoded_tokens, seq_length):\n",
    "        self.encoded_tokens = encoded_tokens\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encoded_tokens) - self.seq_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            torch.tensor(self.encoded_tokens[idx:idx + self.seq_length]),\n",
    "            torch.tensor(self.encoded_tokens[idx + 1:idx + self.seq_length + 1])\n",
    "        )\n",
    "\n",
    "# Dataset and DataLoader\n",
    "seq_length = 5\n",
    "dataset = TextDataset(encoded_tokens, seq_length)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    for src, tgt in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = criterion(output.view(-1, vocab_size), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_next_word(model, sentence, word_to_idx, idx_to_word, max_seq_length):\n",
    "    model.eval()\n",
    "    words = tokenize(preprocess_text(sentence))\n",
    "    input_ids = [word_to_idx[word] for word in words if word in word_to_idx]\n",
    "    src = torch.tensor(input_ids).unsqueeze(0)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(src, src)\n",
    "        next_word_logits = output[0, -1, :]\n",
    "        predicted_word_idx = torch.argmax(next_word_logits).item()\n",
    "        return idx_to_word[predicted_word_idx]\n",
    "\n",
    "# Example prediction\n",
    "sentence = \"Transformers are a type\"\n",
    "predicted_word = predict_next_word(model, sentence, word_to_idx, idx_to_word, max_seq_length)\n",
    "print(f\"Next word prediction: {predicted_word}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
